<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>k8s on kentxxq Blog</title><link>https://kentxxq.com/tags/k8s/</link><description>Recent content in k8s on kentxxq Blog</description><generator>Hugo -- gohugo.io</generator><language>zh</language><lastBuildDate>Mon, 09 Nov 2020 15:40:00 +0800</lastBuildDate><atom:link href="https://kentxxq.com/tags/k8s/index.xml" rel="self" type="application/rss+xml"/><item><title>我的k8s之路0-集群操作</title><link>https://kentxxq.com/contents/%E6%88%91%E7%9A%84k8s%E4%B9%8B%E8%B7%AF0-%E9%9B%86%E7%BE%A4%E6%93%8D%E4%BD%9C/</link><pubDate>Mon, 09 Nov 2020 15:40:00 +0800</pubDate><guid>https://kentxxq.com/contents/%E6%88%91%E7%9A%84k8s%E4%B9%8B%E8%B7%AF0-%E9%9B%86%E7%BE%A4%E6%93%8D%E4%BD%9C/</guid><description>前面写了基本的部署，现在来使用kubespray进行收缩扩容。
前提 参考集群部署中的准备工作 移除kube-master和etcd-master节点。节点配置不能放在hosts中的第一个。例如移除node1，则顺序应为nodeX,node1。 worker节点扩容/替换 文件路径kubespray/inventory/mycluster/hosts.ini，添加对应的机器信息 增加节点scale.yml sudo ansible-playbook -i inventory/mycluster/inventory.ini scale.yml -b -v --limit=kube-node-03 移除节点remove-node.yml # 正常移除节点 sudo ansible-playbook -i inventory/mycluster/inventory.ini remove-node.yml -b -v -e node=kube-node-03 # 如果节点不在线，需要添加参数 sudo ansible-playbook -i inventory/mycluster/inventory.ini remove-node.yml -b -v -e node=kube-node-03 reset_nodes=false 把hosts文件中被移除节点信息去掉 master节点扩容/替换 文件路径kubespray/inventory/mycluster/hosts.ini，添加对应的机器信息 增加节点cluster.yml sudo ansible-playbook -i inventory/mycluster/inventory.ini cluster.yml -b -v 重启服务 # 在每台机器上重启k8s_nginx-proxy_nginx-proxy容器 docker ps | grep k8s_nginx-proxy_nginx-proxy | awk &amp;#39;{print $1}&amp;#39; | xargs docker restart 移除节点 # 正常移除节点 sudo ansible-playbook -i inventory/mycluster/inventory.</description></item><item><title>我的k8s之路0-集群部署</title><link>https://kentxxq.com/contents/%E6%88%91%E7%9A%84k8s%E4%B9%8B%E8%B7%AF0-%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/</link><pubDate>Sat, 07 Nov 2020 20:12:00 +0800</pubDate><guid>https://kentxxq.com/contents/%E6%88%91%E7%9A%84k8s%E4%B9%8B%E8%B7%AF0-%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/</guid><description>之前写了一些k8s的概念和使用，都是操作级别的。而没有自己安装过。这次就来操作一下。同时网上的中文教程都是坑，拍坑是这篇文章一个要点。
简单介绍 虽然说是集群安装教程，但是我还是使用工具来安装。因为这样其实更贴近生产环境的使用。
如果想要了解如何手动部署。那么理解ansible脚本后，自己优化改进脚本会更有帮助。例如不同的linux发行版会有不同的包名、配置等等。这是你在部署时候要注意到的(都在ansible脚本中能找到)。
准备工作 机器准备 3台机器作为集群。 作用 机器 部署机器 本机wsl master 192.168.0.100 Ubuntu 18.04.3 LTS worker 192.168.0.101 Ubuntu 18.04.3 LTS worker 192.168.0.102 Ubuntu 18.04.3 LTS 本机或者另外一台linux作为部署机。如果和我一样使用的是windows，可以使用wsl解决。 部署机环境准备 生成ssh秘钥并免密登录、安装python、安装ansible 在部署机clone当前最新版(release-2.14)kubespray仓库 进入kubespray文件夹，安装依赖(pip install -r requirements.txt) 集群机器环境准备 # 所有操作均由root用户操作 # 如果是centos则应该关闭firewalld，使用yum安装sshpass # 关闭防火墙 ufw disable # 关闭swap交换内存 swapoff -a # 安装软件 apt install sshpass -y 修改ansible脚本 其实最大的坑就是在这里了。如果不了解ansible和具体部署细节，在大陆地区完全无法正常使用。</description></item><item><title>我的k8s之路2-应用部署</title><link>https://kentxxq.com/contents/%E6%88%91%E7%9A%84k8s%E4%B9%8B%E8%B7%AF2-%E5%BA%94%E7%94%A8%E9%83%A8%E7%BD%B2/</link><pubDate>Sat, 01 Aug 2020 11:08:00 +0800</pubDate><guid>https://kentxxq.com/contents/%E6%88%91%E7%9A%84k8s%E4%B9%8B%E8%B7%AF2-%E5%BA%94%E7%94%A8%E9%83%A8%E7%BD%B2/</guid><description>这是k8s的第二篇，但是已经时隔一年了。也不知道是k8s变化了，还是我之前的理解不够正确。 不过我看了一眼，觉得上一篇写的一些内容，还是比较浅显易懂的，作为入门速览还不错。不过从这一篇开始，要好好抠细节了。
pod细节 修正上一篇文章说的pod推荐内部只包含单个docker用法。因为一个pod内部拥有多个docker，在很多场景下是非常不错的解决方案。
首先要说明一件事情，既然pod允许内部包含多个docker，而pod内的资源是可以互访的。没有手动挂在共享内容，是如何实现的呢？
pod内部就是一个小集群。通过infra-container来进行连接，也称之为sidecar设计模式。这样设计有一些有点。
单个image的更新，不会让整个pod重启。从而减少了开销。 解决了pod内容器共享资源的问题。pod内的容器读取到的ip，mac地址等网络相关信息，其实都是infra-container的。 方便拓展功能。例如我要加入一个nginx的转发。拿到一个/a的请求，可以在pod内新增一个用来代理的容器。把请求转发到应用容器的/b上。 ReplicaSet和Deployments 这里一次讲2种kind:ReplicaSet,Deployments。
ReplicaSet是用来创建pod的，控制pod的数量、使用的pod版本。
Deployments是一个无状态的应用。他是用来控制各个版本的ReplicaSet。
通过上面来匹配呢？label。
# 查看运行的pods kubectl get pods # 查看运行的replicasets kubectl get replicasets # 查看运行的deployments kubectl get deployments 实例操作 简单示例 apiVersion: v1 kind: Pod metadata: name: myapp labels: name: myapp spec: containers: - name: myapp image: nginx:1.11.1 resources: limits: memory: &amp;#34;128Mi&amp;#34; cpu: &amp;#34;500m&amp;#34; ports: - containerPort: 80 在上面我们写了一个最简单的pod模板：
一个名为myapp的无状态应用，同时通过label:myapp来匹配内容 指定要用nginx:1.11.1镜像 限制了使用的资源 制定了要对集群暴露的端口 模板示例 模板示例不会把所有的参数都列出来。只是会把日常会用到的写出来。</description></item><item><title>我的k8s之路1-基本概念</title><link>https://kentxxq.com/contents/%E6%88%91%E7%9A%84k8s%E4%B9%8B%E8%B7%AF1-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</link><pubDate>Thu, 04 Jul 2019 11:08:00 +0800</pubDate><guid>https://kentxxq.com/contents/%E6%88%91%E7%9A%84k8s%E4%B9%8B%E8%B7%AF1-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</guid><description>我现在服务器上的微信，博客笔记，redis等等服务都是用的docker部署。在之前写过部署web服务（可拓展），从现在的观点来看，还是比较落伍的，几乎没有自动化。代码部署/系统维护等等的问题没有考虑进去。而docker就解决了这些问题。
而k8s则是docker在企业实践中的事实标准。
我就用大白话来说说自己的理解和使用吧。
关于k8s 每当出现一个问题，就会出现对应的解决方法。下面是我个人的理解的简化版。
openstack是把一堆硬件资源，整合到一起变成一台超级计算机。在此基础上，虚拟出各种资源。对硬件资源进行了整合。同时方便扩容。
docker解决的是app应用的环境问题。
k8s解决了docker编排，负载均衡，自动拓展等等问题。
openstack是为了让vm无忧无虑的健康运行。k8s则是为你的docker提供完美的环境。
概念 这里我仅列出来我暂时用到的概念。总不能一上来，把文档从头看到尾吧～
Pod Pod是k8s的最小计算单元。最常用的情况就是一个docker容器（同时入门使用也推荐此用法）。
如果像是redis集群这样的应用，每个pod之间可以使用卷功能来进行共享数据。
通常在大型应用的场景下，docker的部署很多都是一台机器运行一个docker容器。
而如果你只有3个节点的k8s集群，你也可以运行3个以上的pod。虽然不推荐这样子，但是k8s首要目的是为了让你的部署方案能成功。
pod中的Controller通过Template来创建pod。
Service Service是pod的逻辑集合。对内进行pod的负载均衡。对外提供访问地址。
Volume Volume为pod提供共享数据卷。
Lable Lable用来区分service/pod等资源。LableSelector用来service/pod之间的对应。
Deployment Deployment它管理ReplicaSets和Pod，并提供声明式更新等功能。
一个pod挂了，应用就挂了。而通过Deployment设置用几个pod来提供服务，确保应用正常运作。
web应用 win10的激活可以自己搭建kms服务器。而并不需要数据库来存储任何信息。 StatefulSet StatefulSet用于持久性的应用程序，有唯一的网络标识符（IP），持久存储，有序的部署、扩展、删除和滚动更新。
redis集群 部署应用后一键回滚，且不影响运行中的应用 Job Job用来执行一次性的任务。可定时。
比如开启100个pod来执行一个cpu密集型的应用。完了存到数据库以后，直接销毁。 示例 apiVersion: apps/v1 kind: Deployment metadata: name: azure-vote-back spec: replicas: 1 selector: matchLabels: app: azure-vote-back template: metadata: labels: app: azure-vote-back spec: nodeSelector: &amp;#34;beta.kubernetes.io/os&amp;#34;: linux containers: - name: azure-vote-back image: redis resources: requests: cpu: 100m memory: 128Mi limits: cpu: 250m memory: 256Mi ports: - containerPort: 6379 name: redis --- apiVersion: v1 kind: Service metadata: name: azure-vote-back spec: ports: - port: 6379 selector: app: azure-vote-back --- apiVersion: apps/v1 kind: Deployment metadata: name: azure-vote-front spec: replicas: 1 selector: matchLabels: app: azure-vote-front template: metadata: labels: app: azure-vote-front spec: nodeSelector: &amp;#34;beta.</description></item></channel></rss>